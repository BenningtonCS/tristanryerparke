# Lenet 5 Pytorch experimentation #

Three files, each based off of this tutorial:
https://towardsdatascience.com/implementing-yann-lecuns-lenet-5-in-pytorch-5e05a0911320

lenet-ish_1.py uses the ReLu activation function.
lenet-ish_2.py uses the TanH activation function.
lenet-ish_3.py uses the LeakyReLu activation function.
  
 With the same hyperparameters, all three activation functions generated similar performance at 15 epochs.
